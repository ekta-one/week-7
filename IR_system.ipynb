{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "from math import log\n",
        "\n",
        "# Preprocessing function\n",
        "def preprocess(text):\n",
        "    return re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# Load documents\n",
        "def load_documents(folder_path):\n",
        "    docs = {}\n",
        "    for filename in os.listdir(folder_path):\n",
        "        if filename.endswith('.txt'):\n",
        "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
        "                docs[filename] = file.read()\n",
        "    return docs\n",
        "\n",
        "# Predefined queries\n",
        "predefined_queries = {\n",
        "    1: \"vegetarian recipes with chickpeas and spinach\",\n",
        "    2: \"Chicken Gravy\",\n",
        "    3: \"Mutton Curry\",\n",
        "    4: \"Pizza\",\n",
        "    5: \"high-protein smoothie recipes\"\n",
        "}\n",
        "\n",
        "# Load relevant documents (ground truth for each query)\n",
        "def load_relevances(relevance_file_path):\n",
        "    relevances = defaultdict(set)\n",
        "    with open(relevance_file_path, 'r', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            query_id, doc_id = line.strip().split()\n",
        "            relevances[int(query_id)].add(doc_id)\n",
        "    return relevances\n",
        "\n",
        "# Compute term frequencies and document lengths\n",
        "def compute_tf_and_lengths(documents):\n",
        "    tfs = {}\n",
        "    doc_lengths = {}\n",
        "    for doc_id, text in documents.items():\n",
        "        tokens = preprocess(text)\n",
        "        tf = Counter(tokens)\n",
        "        tfs[doc_id] = tf\n",
        "        doc_lengths[doc_id] = len(tokens)\n",
        "    return tfs, doc_lengths\n",
        "\n",
        "# Compute Inverse Document Frequencies (IDF)\n",
        "def compute_idf(documents):\n",
        "    N = len(documents)\n",
        "    df = defaultdict(int)\n",
        "    for text in documents.values():\n",
        "        tokens = set(preprocess(text))\n",
        "        for token in tokens:\n",
        "            df[token] += 1\n",
        "    idf = {}\n",
        "    for term, freq in df.items():\n",
        "        idf[term] = log((N - freq + 0.5) / (freq + 0.5) + 1)\n",
        "    return idf\n",
        "\n",
        "# Compute BM25 scores\n",
        "def bm25_score(query, tfs, idf, doc_lengths, avg_doc_length, k1=1.5, b=0.75):\n",
        "    query_terms = preprocess(query)\n",
        "    scores = defaultdict(float)\n",
        "    for term in query_terms:\n",
        "        if term not in idf:\n",
        "            continue\n",
        "        for doc_id, tf in tfs.items():\n",
        "            term_freq = tf.get(term, 0)\n",
        "            if term_freq == 0:\n",
        "                continue\n",
        "            doc_len = doc_lengths[doc_id]\n",
        "            numerator = idf[term] * term_freq * (k1 + 1)\n",
        "            denominator = term_freq + k1 * (1 - b + b * (doc_len / avg_doc_length))\n",
        "            scores[doc_id] += numerator / denominator\n",
        "    return scores\n",
        "\n",
        "# Calculate Average Precision (AP)\n",
        "def calculate_average_precision(relevant_docs, ranked_docs):\n",
        "    relevant_docs = set(relevant_docs)\n",
        "    if not relevant_docs:\n",
        "        return 0.0\n",
        "    num_relevant = len(relevant_docs)\n",
        "    score = 0.0\n",
        "    num_hits = 0\n",
        "    for i, (doc_id, _) in enumerate(ranked_docs):\n",
        "        if doc_id in relevant_docs:\n",
        "            num_hits += 1\n",
        "            score += num_hits / (i + 1)\n",
        "    return score / num_relevant\n",
        "\n",
        "# Rank documents with BM25 and calculate AP, saving results to 'result.txt'\n",
        "def rank_documents_bm25(data_dir):\n",
        "    \"\"\" BM25 ranking with Average Precision calculation and result saving. \"\"\"\n",
        "    # Load data\n",
        "    documents = load_documents(os.path.join(data_dir, 'documents'))\n",
        "    relevances = load_relevances(os.path.join(data_dir, 'relevance.txt'))\n",
        "\n",
        "    # Preprocess and compute term frequencies and document lengths\n",
        "    tfs, doc_lengths = compute_tf_and_lengths(documents)\n",
        "    avg_doc_length = sum(doc_lengths.values()) / len(documents)\n",
        "\n",
        "    # Compute IDF for the document collection\n",
        "    idf = compute_idf(documents)\n",
        "\n",
        "    results = {}\n",
        "    total_ap = 0.0\n",
        "\n",
        "    # Open result file\n",
        "    with open('result.txt', 'w', encoding='utf-8') as result_file:\n",
        "        # Score each predefined query against the documents\n",
        "        for query_id, query in predefined_queries.items():\n",
        "            # Compute BM25 scores\n",
        "            scores = bm25_score(query, tfs, idf, doc_lengths, avg_doc_length)\n",
        "\n",
        "            # Sort results\n",
        "            ranked_docs = sorted(scores.items(), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Calculate Average Precision (AP) for this query\n",
        "            ap = calculate_average_precision(relevances.get(query_id, []), ranked_docs)\n",
        "            total_ap += ap\n",
        "\n",
        "            # Write results to file\n",
        "            result_file.write(f\"Query {query_id}: {query}\\n\")\n",
        "            result_file.write(f\"Average Precision: {ap:.4f}\\n\")\n",
        "            result_file.write(\"Top 5 Documents:\\n\")\n",
        "            for doc, score in ranked_docs[:5]:\n",
        "                result_file.write(f\"Document: {doc}, Score: {score:.4f}\\n\")\n",
        "            result_file.write(\"\\n\")\n",
        "\n",
        "        # Compute Mean Average Precision (MAP) across all queries\n",
        "        mean_ap = total_ap / len(predefined_queries)\n",
        "        result_file.write(f\"Mean Average Precision (MAP): {mean_ap:.4f}\\n\")\n",
        "\n",
        "    print(\"Results have been saved to 'result.txt'.\")\n",
        "\n",
        "# Path to the folder containing documents and relevance file\n",
        "folder_path = '/content/Recipie ghar'\n",
        "rank_documents_bm25(folder_path)\n"
      ],
      "metadata": {
        "id": "mRRFgpNNlnIb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad73aeaa-d7f2-42f9-c39a-22e744abe20d"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results have been saved to 'result.txt'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q1-9-tfnKPXs"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}